{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not\n",
      "PyTerrier 0.5.0 has loaded Terrier 5.4 (built by craigm on 2021-01-16 14:17)\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "  print (\"not\")  \n",
    "  pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook/cqas/external_pretrained_models/roberta.hdf5\n",
      "/notebook/cqas/external_pretrained_models/vocab_dir\n",
      "encoder loaded\n",
      "indexer loaded\n",
      "cuda device  cuda:0\n",
      "model path  /notebook/cqas/external_pretrained_models/roberta.hdf5\n",
      "model loaded\n",
      "reader loaded\n",
      "loaded extractors\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# transformers\n",
    "import pytorch_transformers\n",
    "from pytorch_transformers import *\n",
    "\n",
    "# read file\n",
    "from xml.dom import minidom\n",
    "import re\n",
    "\n",
    "#import rank model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# custom extractor\n",
    "import pickle\n",
    "from my_functions import extractorRoberta\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "my_extractor = extractorRoberta(my_device = device, model_path = '/notebook/cqas/external_pretrained_models/')\n",
    "print (\"loaded extractors\")\n",
    "\n",
    "from pyterrier import text\n",
    "from pyterrier.text import scorer\n",
    "\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "  print (\"not\")  \n",
    "  pt.init()\n",
    "\n",
    "def create_featured_dataset(some_df):\n",
    "    from pyterrier import text\n",
    "    from pyterrier.text import scorer\n",
    "    textscorerTf = text.scorer(body_attr=\"text\", wmodel='BM25', sort=False)\n",
    "    rtr_bm = textscorerTf.transform(some_df)\n",
    "    textscorerTf = text.scorer(body_attr=\"text\", wmodel='Tf')\n",
    "    rtr_tf = textscorerTf.transform(some_df)\n",
    "    textscorerTf = text.scorer(body_attr=\"text\", wmodel='PL2')\n",
    "    rtr_pl2 = textscorerTf.transform(some_df)\n",
    "    textscorerTf = text.scorer(body_attr=\"text\", wmodel='DFIC')\n",
    "    rtr_dfic = textscorerTf.transform(some_df)\n",
    "    \n",
    "    rtr_pl2_for_merge = rtr_pl2[['qid', 'docno', 'score']]\n",
    "    rtr_pl2_for_merge = rtr_pl2_for_merge.rename(columns={\"score\": \"score_pl2\"})\n",
    "    \n",
    "    rtr_tf_for_merge = rtr_tf[['qid', 'docno', 'score']]\n",
    "    rtr_tf_for_merge = rtr_tf_for_merge.rename(columns={\"score\": \"score_tf\"})\n",
    "    \n",
    "    rtr_bm_for_merge = rtr_bm[['qid', 'docno', 'score']]\n",
    "    rtr_bm_for_merge = rtr_bm_for_merge.rename(columns={\"score\": \"score_bm\"})\n",
    "    \n",
    "    rtr_dfic_for_merge = rtr_dfic[['qid', 'docno', 'score']]\n",
    "    rtr_dfic_for_merge = rtr_dfic_for_merge.rename(columns={\"score\": \"score_dfic\"})\n",
    "    \n",
    "    result = pd.merge(rtr_pl2_for_merge, rtr_tf_for_merge, on=[\"qid\", \"docno\"])\n",
    "    result = pd.merge(result, rtr_bm_for_merge, on=[\"qid\", \"docno\"])\n",
    "    result = pd.merge(result, rtr_dfic_for_merge, on=[\"qid\", \"docno\"])\n",
    "    result = pd.merge(result, some_df, on=[\"qid\", \"docno\"])\n",
    "    zipped = [result[\"score_pl2\"], result[\"score_tf\"], result[\"score_bm\"], result[\"score_dfic\"], result['baseline_scores'], result[\"is_retrieved\"], result[\"ap_score\"], result[\"objs_score\"]]\n",
    "    unzipped_object = zip(*zipped)\n",
    "    unzipped_list = list(unzipped_object)\n",
    "    list_of_features = [np.array(elem) for elem in unzipped_list]\n",
    "    result['features'] = list_of_features\n",
    "    return result\n",
    "\n",
    "def extract_objs_asp(model_for_extraction, input_string):\n",
    "    model_for_extraction.from_string(input_string)\n",
    "    obj1, obj2, predicates, aspects = model_for_extraction.get_params()\n",
    "    return (obj1.lower(), obj2.lower(), predicates, aspects)\n",
    "\n",
    "\n",
    "def count_score1(text, nlu_tuple):\n",
    "    (obj1, obj2, pred, asp) = nlu_tuple\n",
    "    r = 1.0\n",
    "    if (len(obj1) != 0 and len(obj2) != 0):\n",
    "        if (len(pred) != 0):\n",
    "            pred = re.sub('[!#?,.:\";]', '', pred[0])\n",
    "            if (obj1 in text and obj2 in text and pred in text):\n",
    "                r += 1.0\n",
    "        if (len(asp) != 0):\n",
    "            asp = re.sub('[!#?,.:\";]', '', asp[0])\n",
    "            if (obj1 in text and obj2 in text and asp in text):\n",
    "                r += 1.0\n",
    "        elif (obj1 in text and obj2 in text):\n",
    "            r = 1.5\n",
    "        elif (obj1 in text or obj2 in text):\n",
    "            r = 1.2\n",
    "    else:\n",
    "        if (obj1) in text or (obj2) in text:\n",
    "            r = 1.2\n",
    "    return r\n",
    "\n",
    "def count_score_nlu(nlu_tuple):\n",
    "    if (len(nlu_tuple[0]) == 0):\n",
    "        return 0.0\n",
    "    else: return 1.0\n",
    "\n",
    "\n",
    "def count_score(text, nlu_tuple):\n",
    "    (obj1, obj2, preds, asps) = nlu_tuple\n",
    "    r = 0.0\n",
    "    text = cleanhtml(text)\n",
    "    if (len(obj1) != 0 and len(obj2) != 0):\n",
    "        if (obj1 in text):\n",
    "            r += 1.0\n",
    "        if (obj2 in text):\n",
    "            r += 1.0\n",
    "        for asp in asps:\n",
    "            if asp in text:\n",
    "                r += 1.5\n",
    "        for pred in preds:\n",
    "            if pred in text:\n",
    "                r += 1.0\n",
    "    else:\n",
    "        if ((obj1) in text and len(obj1)!= 0) or (obj2 in text and len(obj2) != 0):\n",
    "            r = 1.0\n",
    "    return r\n",
    "\n",
    "def count_score_obj(text, nlu_tuple):\n",
    "    (obj1, obj2, preds, asps) = nlu_tuple\n",
    "    r = 0.0\n",
    "    text = cleanhtml(text)\n",
    "    if (len(obj1) != 0 and obj1 in text):\n",
    "        r += 1.0\n",
    "    if (len(obj2) != 0 and obj2 in text):\n",
    "        r += 1.0\n",
    "    return r\n",
    "\n",
    "def count_score_asp_pred(text, nlu_tuple):\n",
    "    (obj1, obj2, preds, asps) = nlu_tuple\n",
    "    r = 0.0\n",
    "    text = cleanhtml(text)\n",
    "    o1 = (len(obj1) != 0 and obj1 in text)\n",
    "    o2 = (len(obj2) != 0 and obj2 in text)\n",
    "    if (o1 or o2):\n",
    "        for asp in asps:\n",
    "            if asp in text:\n",
    "                r += 0.5\n",
    "        for pred in preds:\n",
    "            if pred in text:\n",
    "                r += 0.5\n",
    "    return r\n",
    "\n",
    "\n",
    "def make_scores_obj(query, answers):\n",
    "    print (\"make_scores_obj\")\n",
    "    (obj1, obj2, pred, asp) = extract_objs_asp(extr, query)\n",
    "    print (\"in make scores\", obj1, obj2, pred, asp)\n",
    "    scores_answers = [count_score(cleanhtml(answer), (obj1, obj2, pred, asp)) for answer in answers]\n",
    "    return scores_answers\n",
    "\n",
    "def read_xml(filename):\n",
    "    # convert file filename to list of tuples (number_of_topic, title_of_topic) \n",
    "    # input: filename string\n",
    "    # output: list of corresponding tuples\n",
    "    answer_list = []\n",
    "    xmldoc = minidom.parse(filename)\n",
    "    itemlist = xmldoc.getElementsByTagName('topics')\n",
    "    print(len(itemlist))\n",
    "    print(itemlist)\n",
    "    topic_list = itemlist[0].getElementsByTagName('topic')\n",
    "    print (len(topic_list))\n",
    "    for topic in topic_list:\n",
    "        tuple_for_add = tuple((topic.getElementsByTagName('number')[0].firstChild.nodeValue, topic.getElementsByTagName('title')[0].firstChild.nodeValue))\n",
    "        answer_list.append(tuple_for_add)\n",
    "    return answer_list\n",
    "\n",
    "def make_a_search_request(query):\n",
    "    # return json\n",
    "    # json will be processed further\n",
    "    params = {\n",
    "            \"apikey\": \"0833a307-97d3-462a-99d9-27db400c70da\",\n",
    "            \"query\": query,\n",
    "            \"index\": [\"cw12\"],\n",
    "            \"size\": 10,\n",
    "            \"pretty\": True\n",
    "        }\n",
    "    response = requests.get(url = \"http://www.chatnoir.eu/api/v1/_search\", params = params)\n",
    "    return response\n",
    "\n",
    "def clean_punct(s):\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    return s\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#r = requests.post('http://10.30.99.211:8261/gpt_small', data = \"What is better for deep learning Python or Matlab?\")\n",
    "#print (r.status_code)\n",
    "import requests\n",
    "from xml.dom import minidom\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "#numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read retireved documents and qrels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[<DOM Element: topics at 0x7fed801d3cd0>]\n",
      "50\n",
      "1\n",
      "[<DOM Element: topics at 0x7fed516c27d0>]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "topics_2020 = read_xml('topics-task-2.xml')\n",
    "        \n",
    "topics_2021 = read_xml('topics-task-2-only-titles-2021.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('/notebook/touche/list_of_un_answ.pkl', 'rb') as f:\n",
    "    answers_2020 = pickle.load(f)\n",
    "    \n",
    "with open('/notebook/touche2021/touche2020-task2-relevance-withbaseline.qrels', 'r') as f:\n",
    "    qrels_lines = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "qrels = [x.strip().split() for x in qrels_lines] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create qrels dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_dict = {}\n",
    "for elem in qrels:\n",
    "    query, noninf, docno, rank = elem\n",
    "    if (query in qrels_dict.keys()):\n",
    "        qrels_dict[query].append((docno, rank))\n",
    "    else:\n",
    "        qrels_dict[query] = []\n",
    "        qrels_dict[query].append((docno, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clueweb12-1715wb-31-22014', '0'),\n",
       " ('clueweb12-0106wb-15-00715', '0'),\n",
       " ('clueweb12-1313wb-74-02250', '0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_dict['46'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.dataset_readers.dataset_utils import to_bioul\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, Field, MetadataField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.seq2seq_encoders import PassThroughEncoder\n",
    "\n",
    "\n",
    "\n",
    "from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[<DOM Element: topics at 0x7fed516d4870>]\n",
      "50\n",
      "1\n",
      "[<DOM Element: topics at 0x7fed484852d0>]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "topics_2020 = read_xml('topics-task-2.xml')\n",
    "\n",
    "topics_2021 = read_xml('topics-task-2-only-titles-2021.xml')\n",
    "        \n",
    "with open('list_of_un_answ.pcl', 'rb') as f:\n",
    "    answers_2020 = pickle.load(f)\n",
    "            \n",
    "with open('list_of_un_answ_2021.pcl', 'rb') as f:\n",
    "    answers_2021 = pickle.load(f)\n",
    "    \n",
    "with open('touche2020-task2-relevance-withbaseline.qrels', 'r') as f:\n",
    "    qrels_lines = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "qrels = [x.strip().split() for x in qrels_lines] \n",
    "\n",
    "qrels_dict = {}\n",
    "for elem in qrels:\n",
    "    query, noninf, docno, rank = elem\n",
    "    if (query in qrels_dict.keys()):\n",
    "        qrels_dict[query].append((docno, rank))\n",
    "    else:\n",
    "        qrels_dict[query] = []\n",
    "        qrels_dict[query].append((docno, rank))\n",
    "            \n",
    "info_df = pd.DataFrame(columns=[\"qid\", \"query\", \"docno\", \"text\", \"baseline_scores\", \"is_retrieved\", \"ap_score\", \"objs_score\"],dtype=object)\n",
    "info_df_train = pd.DataFrame(columns=[\"qid\", \"query\", \"docno\", \"text\", \"baseline_scores\", \"is_retrieved\", \"ap_score\", \"objs_score\"], dtype=object)\n",
    "qrels_df = pd.DataFrame(columns=[\"qid\", \"docno\", \"label\"],dtype=object)\n",
    "qrels_df_train = pd.DataFrame(columns=[\"qid\", \"docno\", \"label\"],dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ranks1(rtr : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Canonical method for adding a rank column which is calculated based on the score attribute\n",
    "        for each query. Note that the dataframe is NOT sorted by this operation.\n",
    "        Arguments\n",
    "            df: dataframe to create rank attribute for\n",
    "    \"\"\"\n",
    "    rtr.drop(columns=[\"rank\"], errors=\"ignore\", inplace=True)\n",
    "    if len(rtr) == 0:\n",
    "        rtr[\"rank\"] = pd.Series(index=rtr.index, dtype='int64')\n",
    "        return rtr\n",
    "    # -1 assures that first rank will be FIRST_RANK\n",
    "    rtr[\"rank\"] = rtr.groupby(\"qid\", sort=True)[\"score\"].rank(ascending=False, method=\"first\").astype(int) -1 + 1\n",
    "    if True:\n",
    "        print (\"sort\")\n",
    "        rtr.sort_values([\"qid\", \"rank\"], ascending=[True,True], inplace=True)\n",
    "    return rtr\n",
    "\n",
    "def transform(model, test_DF):\n",
    "    \"\"\"\n",
    "    Predicts the scores for the given topics.\n",
    "\n",
    "    Args:\n",
    "        topicsTest(DataFrame): A dataframe with the test topics.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_DF = test_DF.copy()\n",
    "\n",
    "    # check for change in number of features\n",
    "    found_numf = test_DF.iloc[0].features.shape[0]\n",
    "    if model.num_f is not None:\n",
    "        if found_numf != model.num_f:\n",
    "            raise ValueError(\"Expected %d features, but found %d features\" % (model.num_f, found_numf))\n",
    "    if hasattr(model.learner, 'feature_importances_'):\n",
    "        if len(model.learner.feature_importances_) != found_numf:\n",
    "            raise ValueError(\"Expected %d features, but found %d features\" % (len(model.learner.feature_importances_), found_numf))\n",
    "    test_DF[\"score\"] = model.learner.predict(np.stack(test_DF[\"features\"].values))\n",
    "    return add_ranks1(test_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docno</th>\n",
       "      <th>text</th>\n",
       "      <th>baseline_scores</th>\n",
       "      <th>is_retrieved</th>\n",
       "      <th>ap_score</th>\n",
       "      <th>objs_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>clueweb12-0001wb-15-05765</td>\n",
       "      <td>additionally, these medications can become tox...</td>\n",
       "      <td>1984.9130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>clueweb12-0904wb-53-21907</td>\n",
       "      <td>but is your child&amp;#x27;s fever really a cause ...</td>\n",
       "      <td>1700.4545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>clueweb12-1314wb-72-19073</td>\n",
       "      <td>in children, a fever that is equal to or great...</td>\n",
       "      <td>1472.7758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>clueweb12-0311wb-02-29958</td>\n",
       "      <td>it is effective in reducing fever and is also ...</td>\n",
       "      <td>1469.3533</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>clueweb12-0003wb-21-31533</td>\n",
       "      <td>ibuprofen (nurofen and others) is another feve...</td>\n",
       "      <td>1104.5153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                                              query  \\\n",
       "0  51  what is better at reducing fever in children i...   \n",
       "1  51  what is better at reducing fever in children i...   \n",
       "2  51  what is better at reducing fever in children i...   \n",
       "3  51  what is better at reducing fever in children i...   \n",
       "4  51  what is better at reducing fever in children i...   \n",
       "\n",
       "                       docno  \\\n",
       "0  clueweb12-0001wb-15-05765   \n",
       "1  clueweb12-0904wb-53-21907   \n",
       "2  clueweb12-1314wb-72-19073   \n",
       "3  clueweb12-0311wb-02-29958   \n",
       "4  clueweb12-0003wb-21-31533   \n",
       "\n",
       "                                                text  baseline_scores  \\\n",
       "0  additionally, these medications can become tox...        1984.9130   \n",
       "1  but is your child&#x27;s fever really a cause ...        1700.4545   \n",
       "2  in children, a fever that is equal to or great...        1472.7758   \n",
       "3  it is effective in reducing fever and is also ...        1469.3533   \n",
       "4  ibuprofen (nurofen and others) is another feve...        1104.5153   \n",
       "\n",
       "   is_retrieved  ap_score  objs_score  \n",
       "0           1.0       0.0         0.0  \n",
       "1           1.0       0.0         0.0  \n",
       "2           1.0       0.0         0.0  \n",
       "3           1.0       0.0         0.0  \n",
       "4           1.0       1.0         1.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.DataFrame(columns=[\"qid\", \"query\", \"docno\", \"text\", \"baseline_scores\", \"is_retrieved\", \"ap_score\", \"objs_score\"],dtype=object)\n",
    "info_df_train = pd.DataFrame(columns=[\"qid\", \"query\", \"docno\", \"text\", \"baseline_scores\", \"is_retrieved\", \"ap_score\", \"objs_score\"], dtype=object)\n",
    "qrels_df = pd.DataFrame(columns=[\"qid\", \"docno\", \"label\"],dtype=object)\n",
    "qrels_df_train = pd.DataFrame(columns=[\"qid\", \"docno\", \"label\"],dtype=object)\n",
    "\n",
    "for elem in topics_2021:\n",
    "        qid, query = elem[0], elem[1].strip('\\n')\n",
    "        query = re.sub(r'[^\\w\\s]','',query)\n",
    "        query = cleanhtml(query)\n",
    "        my_extractor.from_string(query)\n",
    "        structures = my_extractor.get_params()\n",
    "        \n",
    "        for ind, answer in enumerate(answers_2021[qid]):\n",
    "            docno = answer[1]\n",
    "            score = answer[0]\n",
    "            text = answer[3]\n",
    "\n",
    "            #nlu_score = count_score(text, structures)\n",
    "            objs_score = count_score_obj(text, structures)\n",
    "            ap_score = count_score_asp_pred(text, structures)\n",
    "            is_retrieved = count_score_nlu(structures)\n",
    "            df_row = {\"qid\":int(qid), \"query\":query, \"docno\":docno, \"text\":text, \"baseline_scores\":score, \"is_retrieved\":is_retrieved, \"ap_score\":ap_score, \"objs_score\":objs_score}\n",
    "            info_df = info_df.append(df_row, ignore_index= True)\n",
    "            \n",
    "    #create train df and crels\n",
    "    \n",
    "for elem in topics_2020:\n",
    "    qid, query = elem[0], elem[1].strip('\\n')\n",
    "    query = re.sub(r'[^\\w\\s]','',query)\n",
    "    query = cleanhtml(query)\n",
    "    my_extractor.from_string(query)\n",
    "    structures = my_extractor.get_params()\n",
    "\n",
    "    for ind, answer in enumerate(answers_2020[qid]):\n",
    "        docno = answer[1]\n",
    "        score = answer[0]\n",
    "        text = answer[3]\n",
    "\n",
    "        nlu_score = count_score(text, structures)\n",
    "        objs_score = count_score_obj(text, structures)\n",
    "        ap_score = count_score_asp_pred(text, structures)\n",
    "        is_retrieved = count_score_nlu(structures)\n",
    "        df_row = {\"qid\":int(qid), \"query\":query, \"docno\":docno, \"text\":text, \"baseline_scores\":score, \"is_retrieved\":is_retrieved, \"ap_score\":ap_score, \"objs_score\":objs_score}\n",
    "        info_df_train = info_df_train.append(df_row, ignore_index= True)\n",
    "                \n",
    "    for qrel in qrels_dict[qid]:\n",
    "        docno, label = qrel\n",
    "        df_row = {\"qid\":int(qid), \"docno\":docno, \"label\":label}\n",
    "        qrels_df_train = qrels_df_train.append(df_row, ignore_index= True)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info_df head (2021)  4912   qid                                              query  \\\n",
      "0  51  what is better at reducing fever in children i...   \n",
      "1  51  what is better at reducing fever in children i...   \n",
      "2  51  what is better at reducing fever in children i...   \n",
      "3  51  what is better at reducing fever in children i...   \n",
      "4  51  what is better at reducing fever in children i...   \n",
      "\n",
      "                       docno  \\\n",
      "0  clueweb12-0001wb-15-05765   \n",
      "1  clueweb12-0904wb-53-21907   \n",
      "2  clueweb12-1314wb-72-19073   \n",
      "3  clueweb12-0311wb-02-29958   \n",
      "4  clueweb12-0003wb-21-31533   \n",
      "\n",
      "                                                text  baseline_scores  \\\n",
      "0  additionally, these medications can become tox...        1984.9130   \n",
      "1  but is your child&#x27;s fever really a cause ...        1700.4545   \n",
      "2  in children, a fever that is equal to or great...        1472.7758   \n",
      "3  it is effective in reducing fever and is also ...        1469.3533   \n",
      "4  ibuprofen (nurofen and others) is another feve...        1104.5153   \n",
      "\n",
      "   is_retrieved  ap_score  objs_score  \n",
      "0           1.0       0.0         0.0  \n",
      "1           1.0       0.0         0.0  \n",
      "2           1.0       0.0         0.0  \n",
      "3           1.0       0.0         0.0  \n",
      "4           1.0       1.0         1.0  \n",
      "info_df_test head (2020)  4922   qid                                        query                      docno  \\\n",
      "0   1  what is the difference between sex and love  clueweb12-1214wb-88-29751   \n",
      "1   1  what is the difference between sex and love  clueweb12-1811wb-62-08418   \n",
      "2   1  what is the difference between sex and love  clueweb12-0200wb-79-18105   \n",
      "3   1  what is the difference between sex and love  clueweb12-1311wb-38-04762   \n",
      "4   1  what is the difference between sex and love  clueweb12-0200tw-85-01106   \n",
      "\n",
      "                                                text  baseline_scores  \\\n",
      "0  sex may or may not include penetration. differ...        2406.7341   \n",
      "1  having ’sex’ and a ‘rape’ are two completely d...        2396.6697   \n",
      "2  home &gt; articles &gt; sex, sexuality &amp; p...        2270.9827   \n",
      "3  home &gt;&gt;&gt; sex education 2.0 &gt;&gt;&g...        2096.9185   \n",
      "4  things have changed so much and it has been ye...        2010.6464   \n",
      "\n",
      "   is_retrieved  ap_score  objs_score  \n",
      "0           0.0       0.0         0.0  \n",
      "1           0.0       0.0         0.0  \n",
      "2           0.0       0.0         0.0  \n",
      "3           0.0       0.0         0.0  \n",
      "4           0.0       0.0         0.0  \n",
      "qrels_df_train  1783   qid                      docno label\n",
      "0   1  clueweb12-0001wb-05-12311     0\n",
      "1   1  clueweb12-1811wb-62-08424     1\n",
      "2   1  clueweb12-1811wb-62-08423     1\n",
      "3   1  clueweb12-1217wb-47-14048     0\n",
      "4   1  clueweb12-1811wb-62-08425     1\n"
     ]
    }
   ],
   "source": [
    "print (\"info_df head (2021) \", len(info_df), info_df.head())\n",
    "print (\"info_df_test head (2020) \", len(info_df_train), info_df_train.head())\n",
    "print (\"qrels_df_train \", len(qrels_df_train), qrels_df_train.head())\n",
    "    \n",
    "    \n",
    "test_ds = create_featured_dataset(info_df)\n",
    "result = create_featured_dataset(info_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (info_df.head())\n",
    "#test_ds = create_featured_dataset(info_df)\n",
    "#result = create_featured_dataset(info_df_train)\n",
    "#print (\"\\n\\n test_ds\")\n",
    "#print (test_ds.head())    \n",
    "#rf = RandomForestRegressor(n_estimators=20)\n",
    "#rf_pipe = pt.ltr.apply_learned_model(rf)\n",
    "#rf_pipe.fit(result, qrels_df)\n",
    "#answs = transform(rf_pipe, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# this configures LightGBM as LambdaMART\n",
    "lmart_l = lgb.LGBMRanker(\n",
    "    task=\"train\",\n",
    "    silent=False,\n",
    "    min_data_in_leaf=1,\n",
    "    min_sum_hessian_in_leaf=1,\n",
    "    max_bin=255,\n",
    "    num_leaves=31,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    ndcg_eval_at=[10],\n",
    "    ndcg_at=[10],\n",
    "    eval_at=[10],\n",
    "    learning_rate= .1,\n",
    "    importance_type=\"gain\",\n",
    "    num_iterations=100,\n",
    "    early_stopping_rounds=5\n",
    ")\n",
    "\n",
    "rf_lgbm = pt.ltr.apply_learned_model(lmart_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score_pl2</th>\n",
       "      <th>score_tf</th>\n",
       "      <th>score_bm</th>\n",
       "      <th>score_dfic</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>baseline_scores</th>\n",
       "      <th>is_retrieved</th>\n",
       "      <th>ap_score</th>\n",
       "      <th>objs_score</th>\n",
       "      <th>features</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4165</th>\n",
       "      <td>100</td>\n",
       "      <td>clueweb12-0800wb-21-10162</td>\n",
       "      <td>11.083119</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.942277</td>\n",
       "      <td>23.469861</td>\n",
       "      <td>should i learn python or r for data analysis</td>\n",
       "      <td>i am currently a graduate student in predictiv...</td>\n",
       "      <td>1968.1794</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[11.083119460664996, 6.0, 22.942277059116364, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4166</th>\n",
       "      <td>100</td>\n",
       "      <td>clueweb12-0400wb-22-16393</td>\n",
       "      <td>10.632825</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.066571</td>\n",
       "      <td>20.272496</td>\n",
       "      <td>should i learn python or r for data analysis</td>\n",
       "      <td>using numarray, it is possible to write many e...</td>\n",
       "      <td>1415.7408</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[10.632825001286449, 4.0, 22.066571113474993, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4167</th>\n",
       "      <td>100</td>\n",
       "      <td>clueweb12-1904wb-76-27557</td>\n",
       "      <td>10.300263</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.248211</td>\n",
       "      <td>20.079144</td>\n",
       "      <td>should i learn python or r for data analysis</td>\n",
       "      <td>(note that i am only discussing ease of use fo...</td>\n",
       "      <td>1060.4343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[10.300262903975604, 6.0, 20.248210687936556, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      qid                      docno  score_pl2  score_tf   score_bm  \\\n",
       "4165  100  clueweb12-0800wb-21-10162  11.083119       6.0  22.942277   \n",
       "4166  100  clueweb12-0400wb-22-16393  10.632825       4.0  22.066571   \n",
       "4167  100  clueweb12-1904wb-76-27557  10.300263       6.0  20.248211   \n",
       "\n",
       "      score_dfic                                         query  \\\n",
       "4165   23.469861  should i learn python or r for data analysis   \n",
       "4166   20.272496  should i learn python or r for data analysis   \n",
       "4167   20.079144  should i learn python or r for data analysis   \n",
       "\n",
       "                                                   text  baseline_scores  \\\n",
       "4165  i am currently a graduate student in predictiv...        1968.1794   \n",
       "4166  using numarray, it is possible to write many e...        1415.7408   \n",
       "4167  (note that i am only discussing ease of use fo...        1060.4343   \n",
       "\n",
       "      is_retrieved  ap_score  objs_score  \\\n",
       "4165           1.0       0.0         2.0   \n",
       "4166           1.0       0.0         2.0   \n",
       "4167           1.0       0.0         1.0   \n",
       "\n",
       "                                               features  score  rank  \n",
       "4165  [11.083119460664996, 6.0, 22.942277059116364, ...    0.0     1  \n",
       "4166  [10.632825001286449, 4.0, 22.066571113474993, ...    0.0     2  \n",
       "4167  [10.300262903975604, 6.0, 20.248210687936556, ...    0.0     3  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score_pl2</th>\n",
       "      <th>score_tf</th>\n",
       "      <th>score_bm</th>\n",
       "      <th>score_dfic</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "      <th>baseline_scores</th>\n",
       "      <th>is_retrieved</th>\n",
       "      <th>ap_score</th>\n",
       "      <th>objs_score</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>clueweb12-0001wb-15-05765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>additionally, these medications can become tox...</td>\n",
       "      <td>1984.9130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1984.913, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>clueweb12-0904wb-53-21907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>but is your child&amp;#x27;s fever really a cause ...</td>\n",
       "      <td>1700.4545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1700.4545, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>clueweb12-1314wb-72-19073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>what is better at reducing fever in children i...</td>\n",
       "      <td>in children, a fever that is equal to or great...</td>\n",
       "      <td>1472.7758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1472.7758, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                      docno  score_pl2  score_tf  score_bm  score_dfic  \\\n",
       "0  51  clueweb12-0001wb-15-05765        0.0       0.0       0.0         0.0   \n",
       "1  51  clueweb12-0904wb-53-21907        0.0       0.0       0.0         0.0   \n",
       "2  51  clueweb12-1314wb-72-19073        0.0       0.0       0.0         0.0   \n",
       "\n",
       "                                               query  \\\n",
       "0  what is better at reducing fever in children i...   \n",
       "1  what is better at reducing fever in children i...   \n",
       "2  what is better at reducing fever in children i...   \n",
       "\n",
       "                                                text  baseline_scores  \\\n",
       "0  additionally, these medications can become tox...        1984.9130   \n",
       "1  but is your child&#x27;s fever really a cause ...        1700.4545   \n",
       "2  in children, a fever that is equal to or great...        1472.7758   \n",
       "\n",
       "   is_retrieved  ap_score  objs_score  \\\n",
       "0           1.0       0.0         0.0   \n",
       "1           1.0       0.0         0.0   \n",
       "2           1.0       0.0         0.0   \n",
       "\n",
       "                                         features  \n",
       "0   [0.0, 0.0, 0.0, 0.0, 1984.913, 1.0, 0.0, 0.0]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 1700.4545, 1.0, 0.0, 0.0]  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 1472.7758, 1.0, 0.0, 0.0]  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Should set group for ranking task",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-e0c99e39e717>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_lgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels_df_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics_and_results_Valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrelsValid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqrels_df_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mansws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_lgbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/pyterrier/ltr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, topics_and_results_Train, qrelsTrain, topics_and_results_Valid, qrelsValid)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtrain_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics_and_results_Train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqrelsTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'docno'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_group, eval_metric, eval_at, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# check group data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Should set group for ranking task\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Should set group for ranking task"
     ]
    }
   ],
   "source": [
    "\n",
    "rf_lgbm.fit(result, qrels_df_train, topics_and_results_Valid = result, qrelsValid = qrels_df_train)\n",
    "answs = transform(rf_lgbm, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_qrels(output_dir, name, rtr):\n",
    "    print (\"rtr in write qrels \", rtr.head())\n",
    "    qids = rtr['qid']\n",
    "    Q0s = [0 for elem in qids]\n",
    "    docs = rtr['docno']\n",
    "    scores = rtr['score']\n",
    "    ranks = rtr['rank']\n",
    "    tags = [name for elem in qids]\n",
    "    common_list = list(zip(qids, Q0s, docs, ranks, scores, tags))\n",
    "    print (\"common_list \", common_list[:3])\n",
    "    with open(output_dir + name +'.qrels', 'w') as fp:\n",
    "        fp.write('\\n'.join('%s %s %s %s %s %s' % x for x in common_list))\n",
    "    print (\"written \" + name +'.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = answs.sort_values(\"qid\", ascending=True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dd['qid'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtr in write qrels    qid                      docno  score_pl2  score_tf   score_bm  score_dfic  \\\n",
      "0  51  clueweb12-0504wb-21-02022  16.377481       7.0  34.188286   31.002816   \n",
      "1  51  clueweb12-0604wb-54-09136  15.663725       7.0  33.405079   32.708491   \n",
      "2  51  clueweb12-0505wb-80-31263  13.260739       6.0  27.104483   24.583809   \n",
      "3  51  clueweb12-1908wb-81-21824  12.717186       6.0  26.531794   23.993080   \n",
      "4  51  clueweb12-0410wb-39-29576  12.717186       6.0  26.531794   23.993080   \n",
      "\n",
      "                                               query  \\\n",
      "0  what is better at reducing fever in children i...   \n",
      "1  what is better at reducing fever in children i...   \n",
      "2  what is better at reducing fever in children i...   \n",
      "3  what is better at reducing fever in children i...   \n",
      "4  what is better at reducing fever in children i...   \n",
      "\n",
      "                                                text  baseline_scores  \\\n",
      "0  at home, fevers are more of a nuisance than a ...       1301.04480   \n",
      "1  see: acetaminophen overdose nsaids include asp...        710.59265   \n",
      "2  by 6 hours, children treated with ibuprofen 5m...        449.02432   \n",
      "3  it works by reducing substances in the body th...        570.00920   \n",
      "4  it works by reducing substances in the body th...        558.62994   \n",
      "\n",
      "   is_retrieved  ap_score  objs_score  \\\n",
      "0           1.0       1.0         2.0   \n",
      "1           1.0       1.0         2.0   \n",
      "2           1.0       0.5         1.0   \n",
      "3           1.0       1.0         1.0   \n",
      "4           1.0       1.0         1.0   \n",
      "\n",
      "                                            features  score  rank  \n",
      "0  [16.377480718557777, 7.0, 34.18828622963671, 3...    0.0     1  \n",
      "1  [15.663725156371, 7.0, 33.40507866490724, 32.7...    0.0     2  \n",
      "2  [13.260739414244014, 6.0, 27.104483452764427, ...    0.0     3  \n",
      "3  [12.717186432367184, 6.0, 26.531794229170387, ...    0.0     4  \n",
      "4  [12.717186432367184, 6.0, 26.531794229170387, ...    0.0     5  \n",
      "common_list  [('51', 0, 'clueweb12-0504wb-21-02022', 1, 0.0, 'random_forest'), ('51', 0, 'clueweb12-0604wb-54-09136', 2, 0.0, 'random_forest'), ('51', 0, 'clueweb12-0505wb-80-31263', 3, 0.0, 'random_forest')]\n",
      "written random_forest.txt\n"
     ]
    }
   ],
   "source": [
    "write_qrels('/notebook/touche2021/output/', \"lgbm_lmart\", answs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written standart_test.qrels\n"
     ]
    }
   ],
   "source": [
    "rtr = qrels_df_test\n",
    "qids = rtr['qid']\n",
    "Q0s = [0 for elem in qids]\n",
    "docs = rtr['docno']\n",
    "ranks = rtr['label']\n",
    "\n",
    "common_list = list(zip(qids, Q0s, docs, ranks))\n",
    "with open(\"standart_test\" +'.qrels', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s %s %s %s' % x for x in common_list))\n",
    "print (\"written \" + 'standart' +'_test.qrels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>clueweb12-1608wb-39-15329</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>clueweb12-1700tw-30-03487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>clueweb12-0811wb-62-19220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid                      docno label\n",
       "0  40  clueweb12-1608wb-39-15329     0\n",
       "1  40  clueweb12-1700tw-30-03487     0\n",
       "2  40  clueweb12-0811wb-62-19220     0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_df_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_qrels(\"standart\", qrels_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cord19' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-6e780ff18424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcord19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cord19' is not defined"
     ]
    }
   ],
   "source": [
    "cord19.get_topics(variant='train').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cord19 = pt.datasets.get_dataset('antique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = cord19.get_topics(variant='train')\n",
    "qrels = cord19.get_qrels(variant='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pt_index_path = './terrier_cord19_blocks'\n",
    "\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    # create the index, using the IterDictIndexer indexer \n",
    "    indexer = pt.index.IterDictIndexer(pt_index_path, blocks=True)\n",
    "\n",
    "    # we give the dataset get_corpus_iter() directly to the indexer\n",
    "    # while specifying the fields to index and the metadata to record\n",
    "    index_ref = indexer.index(cord19.get_corpus_iter(), \n",
    "                              fields=('abstract',), \n",
    "                              meta=('docno',))\n",
    "\n",
    "else:\n",
    "    # if you already have the index, use it.\n",
    "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tf = pt.BatchRetrieve(index, wmodel=\"Tf\")\n",
    "pl = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "dfic = pt.BatchRetrieve(index, wmodel=\"DFIC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_va_topics, test_topics = train_test_split(topics, test_size=15, random_state=42)\n",
    "train_topics, valid_topics =  train_test_split(tr_va_topics, test_size=5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_feats1 = (bm25 ** tf ** pl ** dfic)\n",
    "\n",
    "# for reference, lets record the feature names here too\n",
    "fnames=[\"BM25\", \"TF\", 'pl', 'dfic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# this configures XGBoost as LambdaMART\n",
    "lmart_x = xgb.sklearn.XGBRanker(objective='rank:ndcg',\n",
    "      learning_rate=0.1,\n",
    "      gamma=1.0,\n",
    "      min_child_weight=0.1,\n",
    "      max_depth=6,\n",
    "      verbose=2,\n",
    "      random_state=42)\n",
    "\n",
    "rf_lgbm = pt.ltr.apply_learned_model(lmart_x, form=\"ltr\")\n",
    "rf_lgbm.fit(result, qrels_df, topics_and_results_Valid = test_ds, qrelsValid = qrels_df_test)\n",
    "answs = transform(rf_lgbm, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance according LightGBM and XGboost\n",
    "\n",
    "* f0: result[\"score_pl2\"], 5, 1.76\n",
    "\n",
    "* f1: result[\"score_tf\"], 0, 1.19\n",
    "* f2: result[\"score_bm\"], 3, 1.51\n",
    "* f3: result[\"score_dfic\"], 3, 2.3\n",
    "* f4: result['baseline_scores'], 19, 23.82\n",
    "* f5: result[\"is_retrieved\"], 0, 0\n",
    "* f6: result[\"ap_score\"], 3,  1.66\n",
    "* f7: result[\"objs_score\"]0, 1.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LGBMRanker' object has no attribute 'get_booster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-942988f6b2fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_lgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'LGBMRanker' object has no attribute 'get_booster'"
     ]
    }
   ],
   "source": [
    "rf_lgbm.learner.get_booster().get_score(importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1052\n",
      "[LightGBM] [Info] Number of data points in the train set: 4342, number of used features: 8\n",
      "[LightGBM] [Warning] early_stopping_round is set=5, early_stopping_rounds=5 will be ignored. Current value: early_stopping_round=5\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's ndcg@1: 1\tvalid_0's ndcg@2: 1\tvalid_0's ndcg@3: 1\tvalid_0's ndcg@4: 1\tvalid_0's ndcg@5: 1\n",
      "sort\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# this configures LightGBM as LambdaMART\n",
    "lmart_l = lgb.LGBMRanker(\n",
    "    task=\"train\",\n",
    "    silent=False,\n",
    "    min_data_in_leaf=1,\n",
    "    min_sum_hessian_in_leaf=1,\n",
    "    max_bin=255,\n",
    "    num_leaves=12,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    learning_rate= .1,\n",
    "    importance_type=\"gain\",\n",
    "    num_iterations=20,\n",
    "    early_stopping_rounds=5\n",
    ")\n",
    "\n",
    "rf_lgbm = pt.ltr.apply_learned_model(lmart_l, form=\"ltr\")\n",
    "rf_lgbm.fit(result, qrels_df, topics_and_results_Valid = result, qrelsValid = qrels_df)\n",
    "answs = transform(rf_lgbm, test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_lgbm.learner.booster_.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written RF.qrels\n"
     ]
    }
   ],
   "source": [
    "write_qrels(\"RF\", answs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers' score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_custom import make_scores_transformers, run_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head_ in range(11):\n",
    "    run_baseline(head = head_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import onir_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement onir_pt (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for onir_pt\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install onir_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade git+https://github.com/Georgetown-IR-Lab/OpenNIR\n",
    "#!pip install --upgrade git+https://github.com/terrierteam/pyterrier_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbert = onir_pt.reranker('vanilla_transformer', 'bert', text_field='text', vocab_config={'train': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
