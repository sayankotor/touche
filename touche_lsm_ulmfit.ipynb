{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#custom request and preprocessing\n",
    "from help_response import create_list_of_unigue_answers, cleanhtml, clean_punct, read_xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fast ai model\n",
    "import torch\n",
    "\n",
    "from fastai.text.models import AWD_LSTM\n",
    "from fastai.text.learner import get_language_model\n",
    "\n",
    "awd_lstm_lm_config_custom = dict(emb_sz=768, n_hid=1152, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.1, hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2, tie_weights=True, out_bias=True)\n",
    "\n",
    "m = get_language_model(arch=AWD_LSTM, vocab_sz = 60004, config = awd_lstm_lm_config_custom)\n",
    "# state = torch.load('./wt103/fwd_wt103.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SequentialRNN(\n",
       "   (0): AWD_LSTM(\n",
       "     (encoder): Embedding(60004, 768, padding_idx=1)\n",
       "     (encoder_dp): EmbeddingDropout(\n",
       "       (emb): Embedding(60004, 768, padding_idx=1)\n",
       "     )\n",
       "     (rnns): ModuleList(\n",
       "       (0): WeightDropout(\n",
       "         (module): LSTM(768, 1152, batch_first=True)\n",
       "       )\n",
       "       (1): WeightDropout(\n",
       "         (module): LSTM(1152, 1152, batch_first=True)\n",
       "       )\n",
       "       (2): WeightDropout(\n",
       "         (module): LSTM(1152, 768, batch_first=True)\n",
       "       )\n",
       "     )\n",
       "     (input_dp): RNNDropout()\n",
       "     (hidden_dps): ModuleList(\n",
       "       (0): RNNDropout()\n",
       "       (1): RNNDropout()\n",
       "       (2): RNNDropout()\n",
       "     )\n",
       "   )\n",
       "   (1): LinearDecoder(\n",
       "     (decoder): Linear(in_features=768, out_features=60004, bias=True)\n",
       "     (output_dp): RNNDropout()\n",
       "   )\n",
       " ), AWD_LSTM(\n",
       "   (encoder): Embedding(60004, 768, padding_idx=1)\n",
       "   (encoder_dp): EmbeddingDropout(\n",
       "     (emb): Embedding(60004, 768, padding_idx=1)\n",
       "   )\n",
       "   (rnns): ModuleList(\n",
       "     (0): WeightDropout(\n",
       "       (module): LSTM(768, 1152, batch_first=True)\n",
       "     )\n",
       "     (1): WeightDropout(\n",
       "       (module): LSTM(1152, 1152, batch_first=True)\n",
       "     )\n",
       "     (2): WeightDropout(\n",
       "       (module): LSTM(1152, 768, batch_first=True)\n",
       "     )\n",
       "   )\n",
       "   (input_dp): RNNDropout()\n",
       "   (hidden_dps): ModuleList(\n",
       "     (0): RNNDropout()\n",
       "     (1): RNNDropout()\n",
       "     (2): RNNDropout()\n",
       "   )\n",
       " ), Embedding(60004, 768, padding_idx=1), EmbeddingDropout(\n",
       "   (emb): Embedding(60004, 768, padding_idx=1)\n",
       " ), ModuleList(\n",
       "   (0): WeightDropout(\n",
       "     (module): LSTM(768, 1152, batch_first=True)\n",
       "   )\n",
       "   (1): WeightDropout(\n",
       "     (module): LSTM(1152, 1152, batch_first=True)\n",
       "   )\n",
       "   (2): WeightDropout(\n",
       "     (module): LSTM(1152, 768, batch_first=True)\n",
       "   )\n",
       " ), WeightDropout(\n",
       "   (module): LSTM(768, 1152, batch_first=True)\n",
       " ), LSTM(768, 1152, batch_first=True), WeightDropout(\n",
       "   (module): LSTM(1152, 1152, batch_first=True)\n",
       " ), LSTM(1152, 1152, batch_first=True), WeightDropout(\n",
       "   (module): LSTM(1152, 768, batch_first=True)\n",
       " ), LSTM(1152, 768, batch_first=True), RNNDropout(), ModuleList(\n",
       "   (0): RNNDropout()\n",
       "   (1): RNNDropout()\n",
       "   (2): RNNDropout()\n",
       " ), RNNDropout(), RNNDropout(), RNNDropout(), LinearDecoder(\n",
       "   (decoder): Linear(in_features=768, out_features=60004, bias=True)\n",
       "   (output_dp): RNNDropout()\n",
       " ), Linear(in_features=768, out_features=60004, bias=True), RNNDropout()]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = m.modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1ccf7336e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## be sure that model has one AWD_LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih_l0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih_l0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "## be sure that model has one AWD_LSTM\n",
    "assert l[1].rnns[0].module.weight_ih_l0 == l[4][0].module.weight_ih_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as Model\n",
    "from torch import nn\n",
    "\n",
    "class over_AWD_LSTM(nn.Module):\n",
    "    def __init__(self, extra_model, emb_size, ulm_fit_emb_size = 400):\n",
    "        super(over_AWD_LSTM, self).__init__()\n",
    "        self.modules = [module for module in extra_model.modules()]\n",
    "        self.rnns = self.modules[0][0].rnns\n",
    "        self.linear = nn.Linear(emb_size, ulm_fit_emb_size)\n",
    "    def forward(self, input_tensor, from_embeddings = True):\n",
    "        #print (\"input_tensor shape 0 \", input_tensor.shape)\n",
    "        #tsr = self.linear(input_tensor)\n",
    "        #print (\"input_tensor shape 1 \", tsr.shape)\n",
    "        tsr1 = self.rnns[0](input_tensor)\n",
    "        tsr2 = self.rnns[1](tsr1[0])\n",
    "        tsr3 = self.rnns[2](tsr2[0]) # output, (h_n, c_n)\n",
    "        return tsr3 # output, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "bc = BertClient()\n",
    "\n",
    "import re\n",
    "\n",
    "def create_tensor(strng):\n",
    "    #print ((clean_punct(cleanhtml(strng))).split())\n",
    "    arr = bc.encode((clean_punct(cleanhtml(strng))).split())\n",
    "    return torch.tensor(np.copy(arr))\n",
    "    \n",
    "\n",
    "def make_scores_lstm(my_rnn, query, answers, emb_size = 768, hidden_size = 64):\n",
    "    query_embs = bc.encode((clean_punct(cleanhtml(query))).split())\n",
    "    tensor_query_embs = torch.tensor(query_embs)\n",
    "    \n",
    "    all_query_hidden, (last_query_hidden, last_query_state) = my_rnn(tensor_query_embs.unsqueeze(0))\n",
    "    \n",
    "    #last_answers_hidden = [my_rnn(torch.tensor(bc.encode((clean_punct(cleanhtml(answer))).split())).unsqueeze(0))[1] for answer in answers]\n",
    "    last_answers_hidden = [my_rnn(create_tensor(answer).unsqueeze(0))[1][0] for answer in answers]\n",
    "    \n",
    "    aa = last_query_hidden.detach().numpy()\n",
    "    #print (\"aa shape\", aa.shape)\n",
    "    bb = last_answers_hidden[0].detach().numpy()\n",
    "    #print (\"bb shape\", bb.shape)\n",
    "    \n",
    "    scores = [cosine_similarity(last_query_hidden.squeeze(0).detach().numpy(), last_answer_hidden.squeeze(0).detach().numpy())[0][0] for last_answer_hidden in last_answers_hidden]\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy\n",
    "import numpy as np\n",
    "\n",
    "#torch\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#bert service\n",
    "from bert_serving.client import BertClient\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "bc = BertClient()\n",
    "\n",
    "# transformers\n",
    "import pytorch_transformers\n",
    "from pytorch_transformers import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(output_dir = '/notebook/touche/output/', input_dir = '/notebook/touche/', input_file = 'topics-task-2-only-titles.xml'):\n",
    "    \n",
    "    #transformer part\n",
    "    config_class, model_class, tokenizer_class = BertConfig, BertModel, BertTokenizer\n",
    "    config = config_class.from_pretrained('bert-base-uncased')\n",
    "    config.output_attentions=True\n",
    "    model = model_class.from_pretrained('bert-base-uncased', config=config)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    # load responses for all queries from file \n",
    "    my_response_list = create_list_of_unigue_answers(input_dir = input_dir, input_file = input_file)\n",
    "    \n",
    "    list_of_tuples = read_xml(input_dir + input_file)\n",
    "    common_list = []\n",
    "    \n",
    "    awd_lstm_lm_config_custom = dict(emb_sz=768, n_hid=1152, n_layers=3, pad_token=1, qrnn=False, bidir=False, output_p=0.1, hidden_p=0.15, input_p=0.25, embed_p=0.02, weight_p=0.2, tie_weights=True, out_bias=True)\n",
    "    m = get_language_model(arch=AWD_LSTM, vocab_sz = 60004, config = awd_lstm_lm_config_custom)\n",
    "    \n",
    "    my_rnn = over_AWD_LSTM(m, 768)\n",
    "        \n",
    "    print (\"lstm ulm\", flush = True)    \n",
    "    for ind_q, elem in enumerate(list_of_tuples[:5]):\n",
    "        qid = elem[0]\n",
    "        Q0 = 'Q0'\n",
    "        query = elem[1]\n",
    "        tag = 'ULMFIT_LSTM'\n",
    "        responses = list(zip(*my_response_list[str(ind_q + 1)]))\n",
    "\n",
    "        scores0 = responses[0]\n",
    "        #print (\"0\")\n",
    "        docs = responses[1]\n",
    "        #print (\"1\")\n",
    "        titles = responses[2]\n",
    "        #print (\"2\")\n",
    "        answers_bodies = responses[3]\n",
    "        #print (\"3\")\n",
    "        # print (scores0, scores3, scores)\n",
    "        scores = make_scores_lstm(my_rnn, query, answers_bodies)\n",
    "        qids = qid*len(scores)\n",
    "        Q0s = [Q0 for elem in scores]\n",
    "        queries = query*len(scores)\n",
    "        tags = [tag for elem in scores]\n",
    "        part_of_commom_list = list(zip(qids, Q0s, docs, scores, tags))\n",
    "        part_of_commom_list = sorted(part_of_commom_list, key = lambda x: x[3], reverse = True) \n",
    "\n",
    "        qids, Q0s, docs, scores, tags = zip(*part_of_commom_list)\n",
    "\n",
    "        ranks = range(1, len(scores) + 1)\n",
    "        part_of_commom_list = list(zip(qids, Q0s, docs, ranks, scores, tags))\n",
    "        common_list = common_list + part_of_commom_list\n",
    "        \n",
    "    \n",
    "\n",
    "    with open(output_dir + 'run_example_ulm.txt', 'w') as fp:\n",
    "        fp.write('\\n'.join('%s %s %s %s %s %s' % x for x in common_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[<DOM Element: topics at 0x7f788c043d70>]\n",
      "50\n",
      "sze 10\n",
      "sze 100\n",
      "sze 10\n",
      "sze 100\n",
      "sze 10\n",
      "else\n",
      "sze 10\n",
      "sze 100\n",
      "sze 10\n",
      "sze 100\n",
      "1\n",
      "[<DOM Element: topics at 0x7f788c087d70>]\n",
      "50\n",
      "lstm ulm\n"
     ]
    }
   ],
   "source": [
    "run_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
